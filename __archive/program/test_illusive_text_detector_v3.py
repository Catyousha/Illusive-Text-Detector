import fitz
import json
import math
import os
import time
from lorem import text
import sklearn.metrics as metrics

from illusive_text_detector_v3 import IllusiveTextDetector


COLOR_WHITE = 16777215

# Auto generated document for testing still contain unwanted invisible space
# SET TRUE IF DATA COMES FROM AUTO-GENERATED BY PYMUPDF
IGNORE_REAL_SPACE = True 

TEST_DIR = "../test-document-v3"
PREDICTED_JSON_DIR = "./predicted-json"
PREDICTED_TEXT_DIR = "./predicted-text"
PREDICTED_PDF_DIR = "./predicted-pdf"

FONT_TYPE = fitz.Font(ordering=0)
FONT_SIZE = 12

def generate_test_schema():
    """
    [
        {
            'document': 'TEST1.pdf',
            'actual': 'TEST1.json'
        }
    ]
    """
    test_schema = []
    for root, dirs, files in os.walk(TEST_DIR):

        document_files = [f for f in files if f.endswith('.pdf')]
        actual_files = [f for f in files if f.endswith('.json')]
        assert(len(document_files) == len(actual_files))

        test_schema = [
            {
                "document": document_files[i],
                "actual": actual_files[i]
            } for i in range(0, len(document_files))
        ]
    return test_schema

def _comparator(actual, predicted):
    cond1 = math.isclose(actual["origin"][0], predicted["origin"][0], abs_tol=2)
    cond2 = math.isclose(actual["origin"][1], predicted["origin"][1], abs_tol=2)
    cond3 = actual["text"] == predicted["text"]
    assert(cond1 and cond2 and cond3) # PROGRAM IS FAILED IF ERROR
    if(not(cond1 and cond2 and cond3)):
        print(actual, predicted)

    return actual["label"], predicted["label"]
    
def _dump_pred_json(file_name, predicted):
    os.makedirs(PREDICTED_JSON_DIR, exist_ok=True)
    json_dest_file = f"{PREDICTED_JSON_DIR}/PRED_{file_name}"

    with open(json_dest_file, "w") as outfile:
        json_object = json.dumps(predicted, indent=2)
        outfile.write(json_object)

def _dump_pred_text(file_name, predicted):
    os.makedirs(PREDICTED_TEXT_DIR, exist_ok=True)
    txt_dest_file = f"{PREDICTED_TEXT_DIR}/PRED_{file_name}.txt"

    with open(txt_dest_file, 'w') as f:
        text = ""
        for pred in predicted:
            if pred["label"] == "ILLUSIVE":
                text+=" "
            else:
                text+=pred["text"]
        # text = [p["text"] for p in predicted]
        # text = "".join(text)
        f.write(text)

def _dump_pred_pdf(file_name, predicted):
    os.makedirs(PREDICTED_PDF_DIR, exist_ok=True)
    pdf_dest_file = f"{PREDICTED_PDF_DIR}/PRED_{file_name}"

    doc: fitz.Document = fitz.open()
    page: fitz.Page = doc.new_page()
    tw: fitz.TextWriter = fitz.TextWriter(page.rect)
    curr_page = 0
    for pred in predicted:
        if pred["page"] != curr_page:
            tw.write_text(page)
            page = doc.new_page()
            curr_page += 1
        text = pred["text"]
        if(pred["label"] == "ILLUSIVE"):
            text = " "
        tw.append(pred["origin"], text, font=FONT_TYPE, fontsize=FONT_SIZE)
    tw.write_text(page)
    doc.save(pdf_dest_file)



def test_detector():
    detector = IllusiveTextDetector()
    test_schema = generate_test_schema()
    # print(test_schema)
    start_time = time.time()
    count = 0
    sum_acc = 0
    for schema in test_schema:
        actual_json_loc = f"{TEST_DIR}/{schema['actual']}"
        actual_json_file = open(actual_json_loc)
        actual = json.load(actual_json_file)
        actual_json_file.close()

        doc_loc = f"{TEST_DIR}/{schema['document']}"
        predicted = detector.detect(doc_loc)
        predicted.sort(key=lambda p: (p["page"], p["origin"][1], p["origin"][0]))
        
        if IGNORE_REAL_SPACE:
            actual = [a for a in actual if a["text"] != " "]
            predicted = [p for p in predicted if p["text"] != " "]

        _dump_pred_pdf(schema["document"], predicted)
        _dump_pred_json(schema['actual'], predicted)
        _dump_pred_text(schema['actual'].split('.')[0], predicted)

        actual_labels = []
        predicted_labels = []
        for i in range(actual.__len__()):
            a, p = _comparator(actual[i], predicted[i])
            actual_labels.append(a)
            predicted_labels.append(p)

        m = metrics.confusion_matrix(actual_labels, predicted_labels)
        acc = metrics.accuracy_score(actual_labels, predicted_labels)

        count += 1
        sum_acc += acc

        print(f"\n{schema['document']}")
        print(f"COMPLETED IN {time.time() - start_time}")
        print(f"TRUE POSITIVE: {m[0][0]}")
        print(f"FALSE POSITIVE: {m[0][1]}")
        print(f"TRUE NEGATIVE: {m[1][1]}")
        print(f"FALSE NEGATIVE: {m[1][0]}")
        print(f"ACCURACY: {acc}")
        
    print(f"\nDOCUMENT TESTED: {count}")
    print(f"AVG ACC: {sum_acc/count}")

    
if __name__ == "__main__":
    test_detector()